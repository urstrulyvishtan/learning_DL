{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap10/10_5_Convolution_For_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9vk9Elugvmi"
      },
      "source": [
        "# **Notebook 10.5: Convolution for MNIST**\n",
        "\n",
        "This notebook builds a proper network for 2D convolution.  It works with the MNIST dataset (figure 15.15a), which was the original classic dataset for classifying images.  The network will take a 28x28 grayscale image and classify it into one of 10 classes representing a digit.\n",
        "\n",
        "The code is adapted from https://nextjournal.com/gkoehler/pytorch-mnist\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YrXWAH7sUWvU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sibivishtan/anaconda3/envs/learning_dl/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/sibivishtan/anaconda3/envs/learning_dl/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
            "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/sibivishtan/anaconda3/envs/learning_dl/lib/python3.11/site-packages/torchvision/image.so\n",
            "  Expected in:     <D400622C-0C6B-3AE1-AB45-F1D0BF19B384> /Users/sibivishtan/anaconda3/envs/learning_dl/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "/Users/sibivishtan/anaconda3/envs/learning_dl/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wScBGXXFVadm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 6600944.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 906901.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4613756.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3480820.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run this once to load the train and test data straight into a dataloader class\n",
        "# that will provide the batches\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8bKADvLHbiV5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Let's draw some of the training data\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sFvRDGrl4qe"
      },
      "source": [
        "Define the network.  This is a more typical way to define a network than the sequential structure.  We define a class for the network, and define the parameters in the constructor.  Then we use a function called forward to actually run the network.  It's easy to see how you might use residual connections in this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EQkvw2KOPVl7"
      },
      "outputs": [],
      "source": [
        "from os import X_OK\n",
        "# TODO Change this class to implement\n",
        "# 1. A valid convolution with kernel size 5, 1 input channel and 10 output channels\n",
        "# 2. A max pooling operation over a 2x2 area\n",
        "# 3. A Relu\n",
        "# 4. A valid convolution with kernel size 5, 10 input channels and 20 output channels\n",
        "# 5. A 2D Dropout layer\n",
        "# 6. A max pooling operation over a 2x2 area\n",
        "# 7. A relu\n",
        "# 8. A flattening operation\n",
        "# 9. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
        "# 10. A ReLU\n",
        "# 11. A fully connected layer mapping from 50 to 10 dimensions\n",
        "# 12. A softmax function.\n",
        "\n",
        "# Replace this class which implements a minimal network (which still does okay)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Convolutional layer: 1 input channel, 10 output channels, kernel size 5\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=0)  # valid padding\n",
        "        # Max pooling: 2x2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # Convolutional layer: 10 input channels, 20 output channels, kernel size 5\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=0)  # valid padding\n",
        "        # Max pooling: 2x2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        # Dropout for 2D convolutions\n",
        "        self.drop = nn.Dropout2d(p=0.5)  # Example dropout rate of 0.5\n",
        "        # Fully connected layer: size to be determined\n",
        "        self.fc1 = nn.Linear(20 * 4 * 4, 50)  # Adjust the input features after flattening\n",
        "        self.fc2 = nn.Linear(50, 10)  # Final layer to 10 classes\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolutional layer followed by max pooling and ReLU\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # Convolutional layer followed by max pooling, dropout, and ReLU\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.drop(x)\n",
        "        \n",
        "        # Flattening\n",
        "        x = x.flatten(start_dim=1)\n",
        "        \n",
        "        # Fully connected layers with ReLU activation and softmax\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, dim=1)  # Log softmax for classification\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qWZtkCZcU_dg"
      },
      "outputs": [],
      "source": [
        "# He initialization of weights\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FslroPJJffrh"
      },
      "outputs": [],
      "source": [
        "# Create network\n",
        "model = Net()\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xKQd9PzkQ766"
      },
      "outputs": [],
      "source": [
        "# Main training routine\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  # Get each\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Store results\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Byn-f7qWRLxX"
      },
      "outputs": [],
      "source": [
        "# Run on test data\n",
        "def test():\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YgLaex1pfhqz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sibivishtan/anaconda3/envs/learning_dl/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.4004, Accuracy: 1022/10000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000]\tLoss: 2.420823\n",
            "Train Epoch: 1 [640/60000]\tLoss: 2.239226\n",
            "Train Epoch: 1 [1280/60000]\tLoss: 2.148579\n",
            "Train Epoch: 1 [1920/60000]\tLoss: 2.115948\n",
            "Train Epoch: 1 [2560/60000]\tLoss: 2.045696\n",
            "Train Epoch: 1 [3200/60000]\tLoss: 1.842298\n",
            "Train Epoch: 1 [3840/60000]\tLoss: 1.584153\n",
            "Train Epoch: 1 [4480/60000]\tLoss: 1.447806\n",
            "Train Epoch: 1 [5120/60000]\tLoss: 1.353814\n",
            "Train Epoch: 1 [5760/60000]\tLoss: 1.248760\n",
            "Train Epoch: 1 [6400/60000]\tLoss: 0.750071\n",
            "Train Epoch: 1 [7040/60000]\tLoss: 0.946601\n",
            "Train Epoch: 1 [7680/60000]\tLoss: 0.906430\n",
            "Train Epoch: 1 [8320/60000]\tLoss: 0.944059\n",
            "Train Epoch: 1 [8960/60000]\tLoss: 0.638684\n",
            "Train Epoch: 1 [9600/60000]\tLoss: 0.862943\n",
            "Train Epoch: 1 [10240/60000]\tLoss: 0.625984\n",
            "Train Epoch: 1 [10880/60000]\tLoss: 0.571999\n",
            "Train Epoch: 1 [11520/60000]\tLoss: 0.657968\n",
            "Train Epoch: 1 [12160/60000]\tLoss: 0.543199\n",
            "Train Epoch: 1 [12800/60000]\tLoss: 0.453786\n",
            "Train Epoch: 1 [13440/60000]\tLoss: 0.513166\n",
            "Train Epoch: 1 [14080/60000]\tLoss: 0.454447\n",
            "Train Epoch: 1 [14720/60000]\tLoss: 0.482664\n",
            "Train Epoch: 1 [15360/60000]\tLoss: 0.541925\n",
            "Train Epoch: 1 [16000/60000]\tLoss: 0.316052\n",
            "Train Epoch: 1 [16640/60000]\tLoss: 0.524305\n",
            "Train Epoch: 1 [17280/60000]\tLoss: 0.608453\n",
            "Train Epoch: 1 [17920/60000]\tLoss: 0.481140\n",
            "Train Epoch: 1 [18560/60000]\tLoss: 0.387080\n",
            "Train Epoch: 1 [19200/60000]\tLoss: 0.317987\n",
            "Train Epoch: 1 [19840/60000]\tLoss: 0.542102\n",
            "Train Epoch: 1 [20480/60000]\tLoss: 0.415355\n",
            "Train Epoch: 1 [21120/60000]\tLoss: 0.298581\n",
            "Train Epoch: 1 [21760/60000]\tLoss: 0.360389\n",
            "Train Epoch: 1 [22400/60000]\tLoss: 0.566160\n",
            "Train Epoch: 1 [23040/60000]\tLoss: 0.265457\n",
            "Train Epoch: 1 [23680/60000]\tLoss: 0.285855\n",
            "Train Epoch: 1 [24320/60000]\tLoss: 0.366719\n",
            "Train Epoch: 1 [24960/60000]\tLoss: 0.501169\n",
            "Train Epoch: 1 [25600/60000]\tLoss: 0.354671\n",
            "Train Epoch: 1 [26240/60000]\tLoss: 0.173505\n",
            "Train Epoch: 1 [26880/60000]\tLoss: 0.322526\n",
            "Train Epoch: 1 [27520/60000]\tLoss: 0.360832\n",
            "Train Epoch: 1 [28160/60000]\tLoss: 0.450520\n",
            "Train Epoch: 1 [28800/60000]\tLoss: 0.273010\n",
            "Train Epoch: 1 [29440/60000]\tLoss: 0.341829\n",
            "Train Epoch: 1 [30080/60000]\tLoss: 0.157048\n",
            "Train Epoch: 1 [30720/60000]\tLoss: 0.333443\n",
            "Train Epoch: 1 [31360/60000]\tLoss: 0.342850\n",
            "Train Epoch: 1 [32000/60000]\tLoss: 0.367941\n",
            "Train Epoch: 1 [32640/60000]\tLoss: 0.441887\n",
            "Train Epoch: 1 [33280/60000]\tLoss: 0.204249\n",
            "Train Epoch: 1 [33920/60000]\tLoss: 0.323956\n",
            "Train Epoch: 1 [34560/60000]\tLoss: 0.160627\n",
            "Train Epoch: 1 [35200/60000]\tLoss: 0.407537\n",
            "Train Epoch: 1 [35840/60000]\tLoss: 0.282153\n",
            "Train Epoch: 1 [36480/60000]\tLoss: 0.380532\n",
            "Train Epoch: 1 [37120/60000]\tLoss: 0.251062\n",
            "Train Epoch: 1 [37760/60000]\tLoss: 0.218150\n",
            "Train Epoch: 1 [38400/60000]\tLoss: 0.220069\n",
            "Train Epoch: 1 [39040/60000]\tLoss: 0.384603\n",
            "Train Epoch: 1 [39680/60000]\tLoss: 0.253805\n",
            "Train Epoch: 1 [40320/60000]\tLoss: 0.162291\n",
            "Train Epoch: 1 [40960/60000]\tLoss: 0.304692\n",
            "Train Epoch: 1 [41600/60000]\tLoss: 0.329553\n",
            "Train Epoch: 1 [42240/60000]\tLoss: 0.338901\n",
            "Train Epoch: 1 [42880/60000]\tLoss: 0.254665\n",
            "Train Epoch: 1 [43520/60000]\tLoss: 0.243544\n",
            "Train Epoch: 1 [44160/60000]\tLoss: 0.250982\n",
            "Train Epoch: 1 [44800/60000]\tLoss: 0.371713\n",
            "Train Epoch: 1 [45440/60000]\tLoss: 0.198751\n",
            "Train Epoch: 1 [46080/60000]\tLoss: 0.276576\n",
            "Train Epoch: 1 [46720/60000]\tLoss: 0.120894\n",
            "Train Epoch: 1 [47360/60000]\tLoss: 0.194148\n",
            "Train Epoch: 1 [48000/60000]\tLoss: 0.234030\n",
            "Train Epoch: 1 [48640/60000]\tLoss: 0.083883\n",
            "Train Epoch: 1 [49280/60000]\tLoss: 0.076592\n",
            "Train Epoch: 1 [49920/60000]\tLoss: 0.220921\n",
            "Train Epoch: 1 [50560/60000]\tLoss: 0.135815\n",
            "Train Epoch: 1 [51200/60000]\tLoss: 0.282023\n",
            "Train Epoch: 1 [51840/60000]\tLoss: 0.157878\n",
            "Train Epoch: 1 [52480/60000]\tLoss: 0.304400\n",
            "Train Epoch: 1 [53120/60000]\tLoss: 0.225230\n",
            "Train Epoch: 1 [53760/60000]\tLoss: 0.294579\n",
            "Train Epoch: 1 [54400/60000]\tLoss: 0.195710\n",
            "Train Epoch: 1 [55040/60000]\tLoss: 0.202314\n",
            "Train Epoch: 1 [55680/60000]\tLoss: 0.239272\n",
            "Train Epoch: 1 [56320/60000]\tLoss: 0.187723\n",
            "Train Epoch: 1 [56960/60000]\tLoss: 0.173087\n",
            "Train Epoch: 1 [57600/60000]\tLoss: 0.122040\n",
            "Train Epoch: 1 [58240/60000]\tLoss: 0.120169\n",
            "Train Epoch: 1 [58880/60000]\tLoss: 0.276472\n",
            "Train Epoch: 1 [59520/60000]\tLoss: 0.141522\n",
            "\n",
            "Test set: Avg. loss: 0.1244, Accuracy: 9622/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000]\tLoss: 0.439081\n",
            "Train Epoch: 2 [640/60000]\tLoss: 0.289819\n",
            "Train Epoch: 2 [1280/60000]\tLoss: 0.356516\n",
            "Train Epoch: 2 [1920/60000]\tLoss: 0.269493\n",
            "Train Epoch: 2 [2560/60000]\tLoss: 0.362166\n",
            "Train Epoch: 2 [3200/60000]\tLoss: 0.093656\n",
            "Train Epoch: 2 [3840/60000]\tLoss: 0.180127\n",
            "Train Epoch: 2 [4480/60000]\tLoss: 0.220414\n",
            "Train Epoch: 2 [5120/60000]\tLoss: 0.216091\n",
            "Train Epoch: 2 [5760/60000]\tLoss: 0.462722\n",
            "Train Epoch: 2 [6400/60000]\tLoss: 0.107485\n",
            "Train Epoch: 2 [7040/60000]\tLoss: 0.193043\n",
            "Train Epoch: 2 [7680/60000]\tLoss: 0.189999\n",
            "Train Epoch: 2 [8320/60000]\tLoss: 0.185386\n",
            "Train Epoch: 2 [8960/60000]\tLoss: 0.358510\n",
            "Train Epoch: 2 [9600/60000]\tLoss: 0.206683\n",
            "Train Epoch: 2 [10240/60000]\tLoss: 0.200744\n",
            "Train Epoch: 2 [10880/60000]\tLoss: 0.130167\n",
            "Train Epoch: 2 [11520/60000]\tLoss: 0.243230\n",
            "Train Epoch: 2 [12160/60000]\tLoss: 0.159685\n",
            "Train Epoch: 2 [12800/60000]\tLoss: 0.267228\n",
            "Train Epoch: 2 [13440/60000]\tLoss: 0.165442\n",
            "Train Epoch: 2 [14080/60000]\tLoss: 0.479415\n",
            "Train Epoch: 2 [14720/60000]\tLoss: 0.324642\n",
            "Train Epoch: 2 [15360/60000]\tLoss: 0.126660\n",
            "Train Epoch: 2 [16000/60000]\tLoss: 0.161274\n",
            "Train Epoch: 2 [16640/60000]\tLoss: 0.173588\n",
            "Train Epoch: 2 [17280/60000]\tLoss: 0.172180\n",
            "Train Epoch: 2 [17920/60000]\tLoss: 0.302412\n",
            "Train Epoch: 2 [18560/60000]\tLoss: 0.349050\n",
            "Train Epoch: 2 [19200/60000]\tLoss: 0.227248\n",
            "Train Epoch: 2 [19840/60000]\tLoss: 0.136846\n",
            "Train Epoch: 2 [20480/60000]\tLoss: 0.104637\n",
            "Train Epoch: 2 [21120/60000]\tLoss: 0.192480\n",
            "Train Epoch: 2 [21760/60000]\tLoss: 0.611392\n",
            "Train Epoch: 2 [22400/60000]\tLoss: 0.209641\n",
            "Train Epoch: 2 [23040/60000]\tLoss: 0.133481\n",
            "Train Epoch: 2 [23680/60000]\tLoss: 0.097988\n",
            "Train Epoch: 2 [24320/60000]\tLoss: 0.134338\n",
            "Train Epoch: 2 [24960/60000]\tLoss: 0.140632\n",
            "Train Epoch: 2 [25600/60000]\tLoss: 0.304985\n",
            "Train Epoch: 2 [26240/60000]\tLoss: 0.416592\n",
            "Train Epoch: 2 [26880/60000]\tLoss: 0.190206\n",
            "Train Epoch: 2 [27520/60000]\tLoss: 0.296757\n",
            "Train Epoch: 2 [28160/60000]\tLoss: 0.200598\n",
            "Train Epoch: 2 [28800/60000]\tLoss: 0.272241\n",
            "Train Epoch: 2 [29440/60000]\tLoss: 0.202668\n",
            "Train Epoch: 2 [30080/60000]\tLoss: 0.272944\n",
            "Train Epoch: 2 [30720/60000]\tLoss: 0.318249\n",
            "Train Epoch: 2 [31360/60000]\tLoss: 0.165508\n",
            "Train Epoch: 2 [32000/60000]\tLoss: 0.107051\n",
            "Train Epoch: 2 [32640/60000]\tLoss: 0.122957\n",
            "Train Epoch: 2 [33280/60000]\tLoss: 0.142951\n",
            "Train Epoch: 2 [33920/60000]\tLoss: 0.130756\n",
            "Train Epoch: 2 [34560/60000]\tLoss: 0.306615\n",
            "Train Epoch: 2 [35200/60000]\tLoss: 0.163989\n",
            "Train Epoch: 2 [35840/60000]\tLoss: 0.285758\n",
            "Train Epoch: 2 [36480/60000]\tLoss: 0.183567\n",
            "Train Epoch: 2 [37120/60000]\tLoss: 0.133194\n",
            "Train Epoch: 2 [37760/60000]\tLoss: 0.140957\n",
            "Train Epoch: 2 [38400/60000]\tLoss: 0.079808\n",
            "Train Epoch: 2 [39040/60000]\tLoss: 0.135107\n",
            "Train Epoch: 2 [39680/60000]\tLoss: 0.133272\n",
            "Train Epoch: 2 [40320/60000]\tLoss: 0.303552\n",
            "Train Epoch: 2 [40960/60000]\tLoss: 0.255445\n",
            "Train Epoch: 2 [41600/60000]\tLoss: 0.113035\n",
            "Train Epoch: 2 [42240/60000]\tLoss: 0.154547\n",
            "Train Epoch: 2 [42880/60000]\tLoss: 0.242557\n",
            "Train Epoch: 2 [43520/60000]\tLoss: 0.192498\n",
            "Train Epoch: 2 [44160/60000]\tLoss: 0.166402\n",
            "Train Epoch: 2 [44800/60000]\tLoss: 0.379759\n",
            "Train Epoch: 2 [45440/60000]\tLoss: 0.212379\n",
            "Train Epoch: 2 [46080/60000]\tLoss: 0.029704\n",
            "Train Epoch: 2 [46720/60000]\tLoss: 0.162926\n",
            "Train Epoch: 2 [47360/60000]\tLoss: 0.252888\n",
            "Train Epoch: 2 [48000/60000]\tLoss: 0.160606\n",
            "Train Epoch: 2 [48640/60000]\tLoss: 0.109639\n",
            "Train Epoch: 2 [49280/60000]\tLoss: 0.182425\n",
            "Train Epoch: 2 [49920/60000]\tLoss: 0.099106\n",
            "Train Epoch: 2 [50560/60000]\tLoss: 0.181242\n",
            "Train Epoch: 2 [51200/60000]\tLoss: 0.205938\n",
            "Train Epoch: 2 [51840/60000]\tLoss: 0.235600\n",
            "Train Epoch: 2 [52480/60000]\tLoss: 0.286893\n",
            "Train Epoch: 2 [53120/60000]\tLoss: 0.209224\n",
            "Train Epoch: 2 [53760/60000]\tLoss: 0.109252\n",
            "Train Epoch: 2 [54400/60000]\tLoss: 0.157769\n",
            "Train Epoch: 2 [55040/60000]\tLoss: 0.111854\n",
            "Train Epoch: 2 [55680/60000]\tLoss: 0.102044\n",
            "Train Epoch: 2 [56320/60000]\tLoss: 0.149483\n",
            "Train Epoch: 2 [56960/60000]\tLoss: 0.121083\n",
            "Train Epoch: 2 [57600/60000]\tLoss: 0.103280\n",
            "Train Epoch: 2 [58240/60000]\tLoss: 0.229578\n",
            "Train Epoch: 2 [58880/60000]\tLoss: 0.267877\n",
            "Train Epoch: 2 [59520/60000]\tLoss: 0.070729\n",
            "\n",
            "Test set: Avg. loss: 0.0898, Accuracy: 9706/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000]\tLoss: 0.385246\n",
            "Train Epoch: 3 [640/60000]\tLoss: 0.332749\n",
            "Train Epoch: 3 [1280/60000]\tLoss: 0.123320\n",
            "Train Epoch: 3 [1920/60000]\tLoss: 0.229397\n",
            "Train Epoch: 3 [2560/60000]\tLoss: 0.316389\n",
            "Train Epoch: 3 [3200/60000]\tLoss: 0.092361\n",
            "Train Epoch: 3 [3840/60000]\tLoss: 0.180343\n",
            "Train Epoch: 3 [4480/60000]\tLoss: 0.137589\n",
            "Train Epoch: 3 [5120/60000]\tLoss: 0.090894\n",
            "Train Epoch: 3 [5760/60000]\tLoss: 0.319836\n",
            "Train Epoch: 3 [6400/60000]\tLoss: 0.424661\n",
            "Train Epoch: 3 [7040/60000]\tLoss: 0.166866\n",
            "Train Epoch: 3 [7680/60000]\tLoss: 0.063309\n",
            "Train Epoch: 3 [8320/60000]\tLoss: 0.082996\n",
            "Train Epoch: 3 [8960/60000]\tLoss: 0.078484\n",
            "Train Epoch: 3 [9600/60000]\tLoss: 0.112073\n",
            "Train Epoch: 3 [10240/60000]\tLoss: 0.315344\n",
            "Train Epoch: 3 [10880/60000]\tLoss: 0.229959\n",
            "Train Epoch: 3 [11520/60000]\tLoss: 0.181848\n",
            "Train Epoch: 3 [12160/60000]\tLoss: 0.091733\n",
            "Train Epoch: 3 [12800/60000]\tLoss: 0.098745\n",
            "Train Epoch: 3 [13440/60000]\tLoss: 0.042974\n",
            "Train Epoch: 3 [14080/60000]\tLoss: 0.042830\n",
            "Train Epoch: 3 [14720/60000]\tLoss: 0.385747\n",
            "Train Epoch: 3 [15360/60000]\tLoss: 0.156232\n",
            "Train Epoch: 3 [16000/60000]\tLoss: 0.225283\n",
            "Train Epoch: 3 [16640/60000]\tLoss: 0.108378\n",
            "Train Epoch: 3 [17280/60000]\tLoss: 0.130614\n",
            "Train Epoch: 3 [17920/60000]\tLoss: 0.079987\n",
            "Train Epoch: 3 [18560/60000]\tLoss: 0.169295\n",
            "Train Epoch: 3 [19200/60000]\tLoss: 0.103386\n",
            "Train Epoch: 3 [19840/60000]\tLoss: 0.110248\n",
            "Train Epoch: 3 [20480/60000]\tLoss: 0.312949\n",
            "Train Epoch: 3 [21120/60000]\tLoss: 0.076482\n",
            "Train Epoch: 3 [21760/60000]\tLoss: 0.262184\n",
            "Train Epoch: 3 [22400/60000]\tLoss: 0.222220\n",
            "Train Epoch: 3 [23040/60000]\tLoss: 0.153495\n",
            "Train Epoch: 3 [23680/60000]\tLoss: 0.065284\n",
            "Train Epoch: 3 [24320/60000]\tLoss: 0.235916\n",
            "Train Epoch: 3 [24960/60000]\tLoss: 0.172723\n",
            "Train Epoch: 3 [25600/60000]\tLoss: 0.135927\n",
            "Train Epoch: 3 [26240/60000]\tLoss: 0.385361\n",
            "Train Epoch: 3 [26880/60000]\tLoss: 0.137182\n",
            "Train Epoch: 3 [27520/60000]\tLoss: 0.158811\n",
            "Train Epoch: 3 [28160/60000]\tLoss: 0.106393\n",
            "Train Epoch: 3 [28800/60000]\tLoss: 0.142243\n",
            "Train Epoch: 3 [29440/60000]\tLoss: 0.155708\n",
            "Train Epoch: 3 [30080/60000]\tLoss: 0.159415\n",
            "Train Epoch: 3 [30720/60000]\tLoss: 0.244548\n",
            "Train Epoch: 3 [31360/60000]\tLoss: 0.262417\n",
            "Train Epoch: 3 [32000/60000]\tLoss: 0.074377\n",
            "Train Epoch: 3 [32640/60000]\tLoss: 0.091947\n",
            "Train Epoch: 3 [33280/60000]\tLoss: 0.172956\n",
            "Train Epoch: 3 [33920/60000]\tLoss: 0.186063\n",
            "Train Epoch: 3 [34560/60000]\tLoss: 0.022921\n",
            "Train Epoch: 3 [35200/60000]\tLoss: 0.243350\n",
            "Train Epoch: 3 [35840/60000]\tLoss: 0.122533\n",
            "Train Epoch: 3 [36480/60000]\tLoss: 0.137833\n",
            "Train Epoch: 3 [37120/60000]\tLoss: 0.248926\n",
            "Train Epoch: 3 [37760/60000]\tLoss: 0.122532\n",
            "Train Epoch: 3 [38400/60000]\tLoss: 0.070869\n",
            "Train Epoch: 3 [39040/60000]\tLoss: 0.115831\n",
            "Train Epoch: 3 [39680/60000]\tLoss: 0.097231\n",
            "Train Epoch: 3 [40320/60000]\tLoss: 0.335646\n",
            "Train Epoch: 3 [40960/60000]\tLoss: 0.099493\n",
            "Train Epoch: 3 [41600/60000]\tLoss: 0.273606\n",
            "Train Epoch: 3 [42240/60000]\tLoss: 0.157327\n",
            "Train Epoch: 3 [42880/60000]\tLoss: 0.218334\n",
            "Train Epoch: 3 [43520/60000]\tLoss: 0.287908\n",
            "Train Epoch: 3 [44160/60000]\tLoss: 0.086768\n",
            "Train Epoch: 3 [44800/60000]\tLoss: 0.088220\n",
            "Train Epoch: 3 [45440/60000]\tLoss: 0.147600\n",
            "Train Epoch: 3 [46080/60000]\tLoss: 0.371761\n",
            "Train Epoch: 3 [46720/60000]\tLoss: 0.125043\n",
            "Train Epoch: 3 [47360/60000]\tLoss: 0.289522\n",
            "Train Epoch: 3 [48000/60000]\tLoss: 0.118893\n",
            "Train Epoch: 3 [48640/60000]\tLoss: 0.174580\n",
            "Train Epoch: 3 [49280/60000]\tLoss: 0.054151\n",
            "Train Epoch: 3 [49920/60000]\tLoss: 0.086901\n",
            "Train Epoch: 3 [50560/60000]\tLoss: 0.071401\n",
            "Train Epoch: 3 [51200/60000]\tLoss: 0.098510\n",
            "Train Epoch: 3 [51840/60000]\tLoss: 0.193407\n",
            "Train Epoch: 3 [52480/60000]\tLoss: 0.302766\n",
            "Train Epoch: 3 [53120/60000]\tLoss: 0.125104\n",
            "Train Epoch: 3 [53760/60000]\tLoss: 0.072534\n",
            "Train Epoch: 3 [54400/60000]\tLoss: 0.077959\n",
            "Train Epoch: 3 [55040/60000]\tLoss: 0.081474\n",
            "Train Epoch: 3 [55680/60000]\tLoss: 0.079835\n",
            "Train Epoch: 3 [56320/60000]\tLoss: 0.103157\n",
            "Train Epoch: 3 [56960/60000]\tLoss: 0.130060\n",
            "Train Epoch: 3 [57600/60000]\tLoss: 0.116129\n",
            "Train Epoch: 3 [58240/60000]\tLoss: 0.057614\n",
            "Train Epoch: 3 [58880/60000]\tLoss: 0.070071\n",
            "Train Epoch: 3 [59520/60000]\tLoss: 0.126253\n",
            "\n",
            "Test set: Avg. loss: 0.0681, Accuracy: 9770/10000 (98%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get initial performance\n",
        "test()\n",
        "# Train for three epochs\n",
        "n_epochs = 3\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7fRUAy9Se1B"
      },
      "outputs": [],
      "source": [
        "# Run network on data we got before and show predictions\n",
        "output = model(example_data)\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(10):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNAcc98STMeyQgh9SbVHWG+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
